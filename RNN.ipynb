{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import vocab\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')\n",
    "\n",
    "# train val set\n",
    "torch.manual_seed(1)\n",
    "train_dataset, val_dataset = random_split(list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab num: 69023\n"
     ]
    }
   ],
   "source": [
    "# function to remove html tags, keep emoticons, tokenize into words\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "# get list of unique word tokens\n",
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('Vocab num:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4892, 177]\n"
     ]
    }
   ],
   "source": [
    "# sort the unique words\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "# create unique word to unique integer vocab\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "# add two generic vocab\n",
    "vocab.insert_token(\"<pad>\", 0)  # placeholder, for adjusting length of sequence\n",
    "vocab.insert_token(\"<unk>\", 1)  # unknown words, e.g. words that appear in the val or test set but not train set\n",
    "\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "# vocab stores words as integer\n",
    "print([vocab[token] for token in ['hello', 'world']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline to transform each text data into integer vocabs\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "\n",
    "# pipeline to transform label (pos, neg) into (1, 0)\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
    "\n",
    "# function to preprocess (transform to integer vocab) the text in a batch\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "\n",
    "    # for each sample in the batch\n",
    "    for _label, _text in batch:\n",
    "        # convert the label to integer\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # convert the text to integer, and create tensor\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        # track all sample lengths\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # pad samples so each has the same length\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader with the preprocessing function\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_dl = DataLoader(val_dataset, batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # create embedding layer to transform unique integer vocabs to real value vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # one LSTM layer\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "\n",
    "        # one FC hidden layer\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # FC layer output (binary output)\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, vocab, lengths):\n",
    "        # use embedding layer to convert vocab to real value feature vectors\n",
    "        out = self.embedding(vocab)\n",
    "\n",
    "        # pad to same length\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "\n",
    "        # run through the rnn layer\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "\n",
    "        # use the last hidden state from the last hidden layer as input to FC\n",
    "        out = hidden[-1, :, :]\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69025, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train for one epoch\n",
    "def train(dataloader):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    total_acc, total_loss = 0, 0\n",
    "\n",
    "    for vocab_batch, label_batch, lengths in dataloader:\n",
    "        pred = model(vocab_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # metric\n",
    "        total_acc += ((pred > 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader):\n",
    "    # switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    total_acc, total_loss = 0, 0\n",
    "\n",
    "    for vocab_batch, label_batch, lengths in dataloader:\n",
    "        pred = model(vocab_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "\n",
    "        # metric\n",
    "        total_acc += ((pred > 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "num_epochs = 10\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_eval, loss_eval = eval(val_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_eval:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1112350\n",
      "Unique Characters: 80\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "with open('../data/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text = fp.read()\n",
    "\n",
    "# remove non-book stuffs at the beginning\n",
    "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_idx = text.find('End of the Project Gutenberg')\n",
    "text = text[start_idx:end_idx]\n",
    "\n",
    "# get unique characters\n",
    "char_set = set(text)\n",
    "\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape: (1112350,)\n",
      "THE MYSTERIOUS  == Encoding ==> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28] I== Reverse ==>S== Reverse ==>L== Reverse ==>A== Reverse ==>N== Reverse ==>D\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to map unique chars to integers\n",
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch: i for i, ch in enumerate(chars_sorted)}\n",
    "\n",
    "# create a numpy array to map integers back to chars\n",
    "char_array = np.array(chars_sorted)\n",
    "\n",
    "# encode the input text chars to integers\n",
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape:', text_encoded.shape)\n",
    "print(text[:15], '== Encoding ==>', text_encoded[:15])\n",
    "print(text_encoded[15:21], '== Reverse ==>'.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_8432\\2447976048.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40  # every 40 chars as one sequence segment\n",
    "chunk_size = seq_length + 1  # input and target are offset by one character, as we use inputs char i to n to predict i+1 to n+1\n",
    "\n",
    "# sliding window through the input text and create segments\n",
    "text_chunks = [text_encoded[i: i+chunk_size] for i in range(len(text_encoded) - chunk_size)]\n",
    "\n",
    "# create dataset using the segments\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_chunk = self.text_chunks[index]\n",
    "\n",
    "        # input is from index (0, n-1), target is index (1, n)\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "        # create embedding layer to transform unique integer vocabs to real value vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # one LSTM layer\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "\n",
    "        # FC layer output (multiclass output to number of unique chars)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size) \n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # use embedding layer to convert input to real value feature vectors\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "\n",
    "        # run through the rnn layer\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char_array)  # vocab is just the number of unique chars\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3719\n",
      "Epoch 500 loss: 1.5092\n",
      "Epoch 1000 loss: 1.3639\n",
      "Epoch 1500 loss: 1.3212\n",
      "Epoch 2000 loss: 1.1857\n",
      "Epoch 2500 loss: 1.2661\n",
      "Epoch 3000 loss: 1.1597\n",
      "Epoch 3500 loss: 1.1649\n",
      "Epoch 4000 loss: 1.1280\n",
      "Epoch 4500 loss: 1.1791\n",
      "Epoch 5000 loss: 1.1377\n",
      "Epoch 5500 loss: 1.0918\n",
      "Epoch 6000 loss: 1.0983\n",
      "Epoch 6500 loss: 1.0902\n",
      "Epoch 7000 loss: 1.0425\n",
      "Epoch 7500 loss: 1.0642\n",
      "Epoch 8000 loss: 1.0667\n",
      "Epoch 8500 loss: 1.0286\n",
      "Epoch 9000 loss: 1.0535\n",
      "Epoch 9500 loss: 0.9866\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    loss = 0\n",
    "\n",
    "    # get one batch\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "\n",
    "    # predict from every char of each sample of the batch\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss.item() / seq_length\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a sequence of text based on a input text\n",
    "def generateText(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
    "    # encode the input text to integers\n",
    "    encoded_input = torch.tensor([char2int[c] for c in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    # init the output string as the input string (append to it with generated text later)\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "\n",
    "    # for every character in the input string, run through model to update hidden and cell units\n",
    "    for i in range(len(starting_str) - 1):\n",
    "        _, hidden, cell = model(encoded_input[:, i].view(1), hidden, cell)\n",
    "\n",
    "    last_char = encoded_input[:, -1]\n",
    "\n",
    "    for i in range(len_generated_text):\n",
    "        # use the last character in the sequence as the input to model\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "\n",
    "        # process the logits\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor  # less scale = more random, vice versa\n",
    "\n",
    "        # random sample from the logits (so we don't always predict the same character)\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()  # update the last character in the sequence\n",
    "\n",
    "        # append the last character to the generated sequence\n",
    "        generated_str += str(char_array[last_char])\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island is necessary for thity. Cyrus Harding became smoke could scatched him should it\n",
      "crossitious most stream, let us, although? If it threw the convicts thus should be a\n",
      "fan with joy; the captain nor honess was long, sliftly among the vegetation of the forest.\n",
      "\n",
      "There were in shortles, brought the true, took extremity of vapor. But they had to be driven to us there? How could\n",
      "go the banks of January, till they sew datena sound upon unknown for a few cascade,” replied Gideon Spilett, “it is indeed by \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(generateText(model, 'The island'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "492015c89e27cc3e5a8c578383431dd8443ebbc21376cec0740bf401bea83bd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
